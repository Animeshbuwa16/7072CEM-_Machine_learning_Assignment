# -*- coding: utf-8 -*-
"""Animesh_Buwa_ML_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ltsMhbgN_NKao9L0074Sh5pYZmbwNCFn
"""

# 1. Upload the dataset
from google.colab import files
uploaded = files.upload()  # choose har_clean_full.csv when prompted

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier

# 3. Load the clean HAR dataset
df = pd.read_csv("har_clean_full.csv")

print(df.shape)
print(df.head())

# Features & labels
X = df.drop(columns=["subject", "activity"]).values
y = df["activity"].values

# Train-test split (stratified by activity)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

X_train.shape, X_test.shape

# 4. Build models with PCA inside pipelines
def make_models(n_pca_components=50):
    models = {}

    # Logistic Regression
    lr = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=n_pca_components)),
        ("clf", LogisticRegression(max_iter=500, multi_class="multinomial", n_jobs=-1))
    ])
    models["LogisticRegression"] = lr

    # SVM (RBF)
    svm = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=n_pca_components)),
        ("clf", SVC(kernel="rbf"))
    ])
    models["SVM_RBF"] = svm

    # KNN
    knn = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=n_pca_components)),
        ("clf", KNeighborsClassifier())
    ])
    models["KNN"] = knn

    # LDA
    lda = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=n_pca_components)),
        ("clf", LinearDiscriminantAnalysis())
    ])
    models["LDA"] = lda

    # Decision Tree
    dt = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=n_pca_components)),
        ("clf", DecisionTreeClassifier(random_state=42))
    ])
    models["DecisionTree"] = dt

    return models

models = make_models()
list(models.keys())

# 5. Train and evaluate all models
results = []

for name, model in models.items():
    print(f"\n=== Training {name} ===")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"Accuracy ({name}): {acc:.4f}")
    print(classification_report(y_test, y_pred))
    results.append((name, acc))

# Summary
print("\nSummary:")
for name, acc in results:
    print(f"{name}: {acc:.4f}")

def plot_confusion(cm, class_names, title="Confusion matrix"):
    fig, ax = plt.subplots(figsize=(6, 5))
    im = ax.imshow(cm, interpolation="nearest")
    ax.figure.colorbar(im, ax=ax)
    ax.set_title(title)
    ax.set_xticks(np.arange(len(class_names)))
    ax.set_yticks(np.arange(len(class_names)))
    ax.set_xticklabels(class_names, rotation=45, ha="right")
    ax.set_yticklabels(class_names)
    ax.set_ylabel("True label")
    ax.set_xlabel("Predicted label")
    plt.tight_layout()
    plt.show()

results = []
best_models = {}
best_params_all = {}

cv_folds = 5  # you can change to 3 for speed if needed

# Redefine models with param_grids to fix the "too many values to unpack" error
def make_models_for_grid_search(n_pca_components=50):
    tuned_models = {}

    # Logistic Regression
    lr_pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=n_pca_components)),
        ("clf", LogisticRegression(max_iter=500, multi_class="multinomial", n_jobs=-1))
    ])
    lr_param_grid = {
        "clf__C": [0.1, 1, 10],
        "pca__n_components": [30, 50, 70] # Example grid search for PCA components too
    }
    tuned_models["LogisticRegression"] = (lr_pipe, lr_param_grid)

    # SVM (RBF)
    svm_pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=n_pca_components)),
        ("clf", SVC(kernel="rbf"))
    ])
    svm_param_grid = {
        "clf__C": [0.1, 1, 10],
        "clf__gamma": [0.01, 0.1, 1],
        "pca__n_components": [30, 50, 70]
    }
    tuned_models["SVM_RBF"] = (svm_pipe, svm_param_grid)

    # KNN
    knn_pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=n_pca_components)),
        ("clf", KNeighborsClassifier())
    ])
    knn_param_grid = {
        "clf__n_neighbors": [3, 5, 7],
        "clf__weights": ["uniform", "distance"],
        "pca__n_components": [30, 50, 70]
    }
    tuned_models["KNN"] = (knn_pipe, knn_param_grid)

    # LDA (No grid search for LDA parameters for simplicity, but PCA can be tuned)
    lda_pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=n_pca_components)),
        ("clf", LinearDiscriminantAnalysis())
    ])
    lda_param_grid = {
        "pca__n_components": [30, 50, 70]
    } # Keep it empty if no params to tune, or tune PCA
    tuned_models["LDA"] = (lda_pipe, lda_param_grid)

    # Decision Tree
    dt_pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=n_pca_components)),
        ("clf", DecisionTreeClassifier(random_state=42))
    ])
    dt_param_grid = {
        "clf__max_depth": [None, 10, 20, 30],
        "clf__min_samples_split": [2, 5, 10],
        "pca__n_components": [30, 50, 70]
    }
    tuned_models["DecisionTree"] = (dt_pipe, dt_param_grid)

    return tuned_models

models = make_models_for_grid_search()

for name, (pipe, param_grid) in models.items():
    print(f"\n====================")
    print(f"Training model: {name}")
    print(f"====================")

    if param_grid:  # use GridSearch if we have a param grid
        grid = GridSearchCV(
            estimator=pipe,
            param_grid=param_grid,
            cv=cv_folds,
            n_jobs=-1,
            scoring="accuracy",
            verbose=1
        )
        grid.fit(X_train, y_train)
        best_model = grid.best_estimator_
        best_params = grid.best_params_
        print(f"Best params for {name}: {best_params}")
        best_params_all[name] = best_params
    else:
        # If no param_grid, just fit the pipeline
        pipe.fit(X_train, y_train)
        best_model = pipe
        best_params_all[name] = {} # No best params if no grid search

    # Store the best model
    best_models[name] = best_model

    # Evaluate on test set
    y_pred = best_model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"\nTest Accuracy for {name}: {acc:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    results.append((name, acc))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))
    plot_confusion(cm, class_names=np.unique(y_test), title=f"Confusion Matrix - {name}")

print("\n=========== SUMMARY ===========")
for name, acc in sorted(results, key=lambda x: x[1], reverse=True):
    print(f"{name:20s}: {acc:.4f}")

print("\nBest hyperparameters per model:")
for name, params in best_params_all.items():
    print(f"{name}: {params}")

# Optional: PCA 2D scatter of the whole dataset, coloured by activity
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca_vis = PCA(n_components=2)
X_pca_vis = pca_vis.fit_transform(X_scaled)

plt.figure(figsize=(7, 6))
for activity in np.unique(y):
    idx = (y == activity)
    plt.scatter(X_pca_vis[idx, 0], X_pca_vis[idx, 1], s=10, alpha=0.6, label=activity)

plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA (2D) projection of HAR features")
plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()